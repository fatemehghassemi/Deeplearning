{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-2-LrFinder-Hoda.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOnKkYTxpP8X2aBY+T69yY+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatemehghassemi/Deeplearning/blob/main/3_2_LrFinder_Hoda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO18jVEYIKcr"
      },
      "source": [
        "# import the necessary packages\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tempfile\n",
        "\n",
        "class LearningRateFinder:\n",
        "\tdef __init__(self, model, stopFactor=4, beta=0.98):\n",
        "\t\t# store the model, stop factor, and beta value (for computing\n",
        "\t\t# a smoothed, average loss)\n",
        "\t\tself.model = model\n",
        "\t\tself.stopFactor = stopFactor\n",
        "\t\tself.beta = beta\n",
        "\n",
        "\t\t# initialize our list of learning rates and losses,\n",
        "\t\t# respectively\n",
        "\t\tself.lrs = []\n",
        "\t\tself.losses = []\n",
        "\n",
        "\t\t# initialize our learning rate multiplier, average loss, best\n",
        "\t\t# loss found thus far, current batch number, and weights file\n",
        "\t\tself.lrMult = 1\n",
        "\t\tself.avgLoss = 0\n",
        "\t\tself.bestLoss = 1e9\n",
        "\t\tself.batchNum = 0\n",
        "\t\tself.weightsFile = None\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\t# re-initialize all variables from our constructor\n",
        "\t\tself.lrs = []\n",
        "\t\tself.losses = []\n",
        "\t\tself.lrMult = 1\n",
        "\t\tself.avgLoss = 0\n",
        "\t\tself.bestLoss = 1e9\n",
        "\t\tself.batchNum = 0\n",
        "\t\tself.weightsFile = None\n",
        "\n",
        "\tdef is_data_iter(self, data):\n",
        "\t\t# define the set of class types we will check for\n",
        "\t\titerClasses = [\"NumpyArrayIterator\", \"DirectoryIterator\",\n",
        "\t\t\t \"Iterator\", \"Sequence\"]\n",
        "\n",
        "\t\t# return whether our data is an iterator\n",
        "\t\treturn data.__class__.__name__ in iterClasses\n",
        "\n",
        "\tdef on_batch_end(self, batch, logs):\n",
        "\t\t# grab the current learning rate and add log it to the list of\n",
        "\t\t# learning rates that we've tried\n",
        "\t\tlr = K.get_value(self.model.optimizer.lr)\n",
        "\t\tself.lrs.append(lr)\n",
        "\n",
        "\t\t# grab the loss at the end of this batch, increment the total\n",
        "\t\t# number of batches processed, compute the average average\n",
        "\t\t# loss, smooth it, and update the losses list with the\n",
        "\t\t# smoothed value\n",
        "\t\tl = logs[\"loss\"]\n",
        "\t\tself.batchNum += 1\n",
        "\t\tself.avgLoss = (self.beta * self.avgLoss) + ((1 - self.beta) * l)\n",
        "\t\tsmooth = self.avgLoss / (1 - (self.beta ** self.batchNum))\n",
        "\t\tself.losses.append(smooth)\n",
        "\n",
        "\t\t# compute the maximum loss stopping factor value\n",
        "\t\tstopLoss = self.stopFactor * self.bestLoss\n",
        "\n",
        "\t\t# check to see whether the loss has grown too large\n",
        "\t\tif self.batchNum > 1 and smooth > stopLoss:\n",
        "\t\t\t# stop returning and return from the method\n",
        "\t\t\tself.model.stop_training = True\n",
        "\t\t\treturn\n",
        "\n",
        "\t\t# check to see if the best loss should be updated\n",
        "\t\tif self.batchNum == 1 or smooth < self.bestLoss:\n",
        "\t\t\tself.bestLoss = smooth\n",
        "\n",
        "\t\t# increase the learning rate\n",
        "\t\tlr *= self.lrMult\n",
        "\t\tK.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "\tdef find(self, trainData, startLR, endLR, epochs=None,\n",
        "\t\tstepsPerEpoch=None, batchSize=32, sampleSize=2048,\n",
        "\t\tverbose=1):\n",
        "\t\t# reset our class-specific variables\n",
        "\t\tself.reset()\n",
        "\n",
        "\t\t# determine if we are using a data generator or not\n",
        "\t\tuseGen = self.is_data_iter(trainData)\n",
        "\n",
        "\t\t# if we're using a generator and the steps per epoch is not\n",
        "\t\t# supplied, raise an error\n",
        "\t\tif useGen and stepsPerEpoch is None:\n",
        "\t\t\tmsg = \"Using generator without supplying stepsPerEpoch\"\n",
        "\t\t\traise Exception(msg)\n",
        "\n",
        "\t\t# if we're not using a generator then our entire dataset must\n",
        "\t\t# already be in memory\n",
        "\t\telif not useGen:\n",
        "\t\t\t# grab the number of samples in the training data and\n",
        "\t\t\t# then derive the number of steps per epoch\n",
        "\t\t\tnumSamples = len(trainData[0])\n",
        "\t\t\tstepsPerEpoch = np.ceil(numSamples / float(batchSize))\n",
        "\n",
        "\t\t# if no number of training epochs are supplied, compute the\n",
        "\t\t# training epochs based on a default sample size\n",
        "\t\tif epochs is None:\n",
        "\t\t\tepochs = int(np.ceil(sampleSize / float(stepsPerEpoch)))\n",
        "\n",
        "\t\t# compute the total number of batch updates that will take\n",
        "\t\t# place while we are attempting to find a good starting\n",
        "\t\t# learning rate\n",
        "\t\tnumBatchUpdates = epochs * stepsPerEpoch\n",
        "\n",
        "\t\t# derive the learning rate multiplier based on the ending\n",
        "\t\t# learning rate, starting learning rate, and total number of\n",
        "\t\t# batch updates\n",
        "\t\tself.lrMult = (endLR / startLR) ** (1.0 / numBatchUpdates)\n",
        "\n",
        "\t\t# create a temporary file path for the model weights and\n",
        "\t\t# then save the weights (so we can reset the weights when we\n",
        "\t\t# are done)\n",
        "\t\tself.weightsFile = tempfile.mkstemp()[1]\n",
        "\t\tself.model.save_weights(self.weightsFile)\n",
        "\n",
        "\t\t# grab the *original* learning rate (so we can reset it\n",
        "\t\t# later), and then set the *starting* learning rate\n",
        "\t\torigLR = K.get_value(self.model.optimizer.lr)\n",
        "\t\tK.set_value(self.model.optimizer.lr, startLR)\n",
        "\n",
        "\t\t# construct a callback that will be called at the end of each\n",
        "\t\t# batch, enabling us to increase our learning rate as training\n",
        "\t\t# progresses\n",
        "\t\tcallback = LambdaCallback(on_batch_end=lambda batch, logs:\n",
        "\t\t\tself.on_batch_end(batch, logs))\n",
        "\n",
        "\t\t# check to see if we are using a data iterator\n",
        "\t\tif useGen:\n",
        "\t\t\tself.model.fit_generator(\n",
        "\t\t\t\ttrainData,\n",
        "\t\t\t\tsteps_per_epoch=stepsPerEpoch,\n",
        "\t\t\t\tepochs=epochs,\n",
        "\t\t\t\tverbose=verbose,\n",
        "\t\t\t\tcallbacks=[callback])\n",
        "\n",
        "\t\t# otherwise, our entire training data is already in memory\n",
        "\t\telse:\n",
        "\t\t\t# train our model using Keras' fit method\n",
        "\t\t\tself.model.fit(\n",
        "\t\t\t\ttrainData[0], trainData[1],\n",
        "\t\t\t\tbatch_size=batchSize,\n",
        "\t\t\t\tepochs=epochs,\n",
        "\t\t\t\tcallbacks=[callback],\n",
        "\t\t\t\tverbose=verbose)\n",
        "\n",
        "\t\t# restore the original model weights and learning rate\n",
        "\t\tself.model.load_weights(self.weightsFile)\n",
        "\t\tK.set_value(self.model.optimizer.lr, origLR)\n",
        "\n",
        "\tdef plot_loss(self, skipBegin=10, skipEnd=1, title=\"\"):\n",
        "\t\t# grab the learning rate and losses values to plot\n",
        "\t\tlrs = self.lrs[skipBegin:-skipEnd]\n",
        "\t\tlosses = self.losses[skipBegin:-skipEnd]\n",
        "\n",
        "\t\t# plot the learning rate vs. loss\n",
        "\t\tplt.plot(lrs, losses)\n",
        "\t\tplt.xscale(\"log\")\n",
        "\t\tplt.xlabel(\"Learning Rate (Log Scale)\")\n",
        "\t\tplt.ylabel(\"Loss\")\n",
        "\n",
        "\t\t# if the title is not empty, add it to the plot\n",
        "\t\tif title != \"\":\n",
        "\t\t\tplt.title(title)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh331lJhJEs6",
        "outputId": "b6ebab98-f7a4-4002-e4c8-76d184324102"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/Alireza-Akhavan/SRU-deeplearning-workshop/master/dataset.py\n",
        "!mkdir dataset\n",
        "!wget https://github.com/Alireza-Akhavan/SRU-deeplearning-workshop/raw/master/dataset/Data_hoda_full.mat -P dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-24 08:33:57--  https://raw.githubusercontent.com/Alireza-Akhavan/SRU-deeplearning-workshop/master/dataset.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 929 [text/plain]\n",
            "Saving to: ‘dataset.py.1’\n",
            "\n",
            "\rdataset.py.1          0%[                    ]       0  --.-KB/s               \rdataset.py.1        100%[===================>]     929  --.-KB/s    in 0s      \n",
            "\n",
            "2021-06-24 08:33:57 (48.4 MB/s) - ‘dataset.py.1’ saved [929/929]\n",
            "\n",
            "mkdir: cannot create directory ‘dataset’: File exists\n",
            "--2021-06-24 08:33:57--  https://github.com/Alireza-Akhavan/SRU-deeplearning-workshop/raw/master/dataset/Data_hoda_full.mat\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Alireza-Akhavan/SRU-deeplearning-workshop/master/dataset/Data_hoda_full.mat [following]\n",
            "--2021-06-24 08:33:58--  https://raw.githubusercontent.com/Alireza-Akhavan/SRU-deeplearning-workshop/master/dataset/Data_hoda_full.mat\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3989009 (3.8M) [application/octet-stream]\n",
            "Saving to: ‘dataset/Data_hoda_full.mat.1’\n",
            "\n",
            "Data_hoda_full.mat. 100%[===================>]   3.80M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-06-24 08:33:58 (89.9 MB/s) - ‘dataset/Data_hoda_full.mat.1’ saved [3989009/3989009]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13uHV8DGJLUI"
      },
      "source": [
        "from dataset import load_hoda\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import os\n",
        "import datetime"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0er8-m6Cfv4x"
      },
      "source": [
        "x_train_original, y_train_original, x_test_original, y_test_original = load_hoda(\n",
        "                                                                        training_sample_size=3500,\n",
        "                                                                        test_sample_size=400,size=28)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mle3mNesfzNv"
      },
      "source": [
        "x_train = np.array(x_train_original)\n",
        "x_test = np.array(x_test_original)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaWdbV0ggISx"
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjzMdxGegLRw"
      },
      "source": [
        "x_train = x_train.reshape(-1,28,28,1)\n",
        "x_test = x_test.reshape(-1,28,28,1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZaWSPpRgOWv"
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train_original, num_classes=10)\n",
        "y_test = keras.utils.to_categorical(y_test_original, num_classes=10)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFayVGuigSLQ"
      },
      "source": [
        "x_val = x_test[:200]\n",
        "x_test = x_test[200:]\n",
        "y_val = y_test[:200]\n",
        "y_test = y_test[200:]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTr4elRyhuEM"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6no1GIQJpXL"
      },
      "source": [
        "MIN_LR = 1e-5"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN8MgyeGJtS2",
        "outputId": "35f4ec04-28f1-4aeb-c0cf-248a1b8cb0e1"
      },
      "source": [
        "opt = tf.keras.optimizers.SGD(lr=MIN_LR, momentum=0.9)\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "id": "G0kb9WdmJy_3",
        "outputId": "314246e3-7b8f-4619-8efa-12757ad09d12"
      },
      "source": [
        "# initialize the learning rate finder and then train with learning\n",
        "# rates ranging from 1e-10 to 1e+1\n",
        "lrf = LearningRateFinder(model)\n",
        "lrf.find((x_train, y_train),1e-10, 1e+1)\n",
        "# plot the loss for the various learning rates and save the\n",
        "# resulting plot to disk\n",
        "lrf.plot_loss()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a68559de4fdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# rates ranging from 1e-10 to 1e+1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearningRateFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e-10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e+1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# plot the loss for the various learning rates and save the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# resulting plot to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-c3b2805f573c>\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, trainData, startLR, endLR, epochs, stepsPerEpoch, batchSize, sampleSize, verbose)\u001b[0m\n\u001b[1;32m    150\u001b[0m                                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m \t\t\t\tverbose=verbose)\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;31m# restore the original model weights and learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 _r=1):\n\u001b[1;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  logits and labels must have the same first dimension, got logits shape [32,10] and labels shape [320]\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at /usr/local/lib/python3.7/dist-packages/keras/backend.py:4945) ]] [Op:__inference_train_function_780]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:\n sparse_categorical_crossentropy/Reshape (defined at /usr/local/lib/python3.7/dist-packages/keras/backend.py:3400)\t\n sparse_categorical_crossentropy/Reshape_1 (defined at /usr/local/lib/python3.7/dist-packages/keras/backend.py:4940)\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    }
  ]
}